---
title: "Regresión Lineal Simple Bayesiana"
format:
  dashboard:
    code-fold: false
    scrolling: true
    orientation: rows
server: shiny
---

```{r}
#| context: setup
library(data.table)
library(ggplot2)
library(GGally)
library(viridis)
library(gridExtra)
library(invgamma)
library(HDInterval)
library(MASS)

plot_bayes_regression <- function(y, X, posterior_samples, 
                                  beta_bar, A, nu0, s0_sq,  # Se agregan hiperparámetros
                                  alpha = 0.95, 
                                  use_theoretical = FALSE) 
                                  {
    
    library(ggplot2)
    library(HDInterval)
    
    # Validar que solo hay un predictor más intercepto
    if(ncol(posterior_samples$beta) != 2) {
        stop("La función solo funciona para modelos con intercepto y un predictor")
    }

    X <- cbind(1, X)
    n <- nrow(X)
    k <- 2
    # Crear data frame para ggplot
    data_df <- data.frame(y = y, x = X[,2])  # Asume que X ya incluye intercepto
    
    # Secuencia de x para predicción
    x_seq <- seq(min(data_df$x), max(data_df$x), length.out = n)
    
    # Matriz de diseño para predicción
    X_pred <- cbind(1, x_seq)
    
    # Calcular estimación teórica exacta (usando prior conjugado)
    if(use_theoretical) {
        
        XtX <- crossprod(X)
        XtX_A <- XtX + A
        XtX_A_inv <- solve(XtX_A)
        
        beta_hat <- XtX_A_inv %*% (XtX %*% beta_bar + crossprod(X, y))
        
        S <- nu0 * s0_sq + crossprod(y) - t(beta_bar) %*% XtX %*% beta_bar
        sigma2_hat <- S / (nrow(X) + nu0)  # Media de la distribución inversa-gamma
        
        theoretical_fit <- X_pred %*% beta_hat
    }
    
    # Media posterior de beta
    beta_post_mean <- colMeans(posterior_samples$beta)
    
    # Generar distribuciones predictivas
    post_pred <- X_pred %*% t(posterior_samples$beta)
    
    # Calcular HDI
    hdi_bounds <- apply(post_pred, 1, function(x) hdi(x, credMass = alpha))
    
    # Crear data frame para gráfico
    plot_df <- data.frame(
        x = x_seq,
        post_mean = rowMeans(post_pred),
        hdi_lower = hdi_bounds[1,],
        hdi_upper = hdi_bounds[2,]
    )
    
    if(use_theoretical) plot_df$theoretical <- theoretical_fit
    
    # Crear gráfico base
    gg <- ggplot(data_df, aes(x = x, y = y)) +
        geom_point(alpha = 0.5, color = "#4B0082") +
        geom_line(data = plot_df, aes(y = post_mean, color = "Muestreo"),
                  linewidth = 1.2) +
        geom_ribbon(data = plot_df, aes(ymin = hdi_lower, ymax = hdi_upper),
                    fill = "#2F4F4F", alpha = 0.2) +
        labs(
            title = "",
            y = expression(log(frac(plain(B)[1], plain(B)[2]))),
            x = expression(log(frac(plain(R)[1], plain(R)[2]))),
            color = "Estimación") +  # Añadir título de la leyenda
        theme_minimal(base_size = 14) +
        theme(legend.position = "top")
    
    # Añadir línea teórica si se solicita
    if(use_theoretical) {
        gg <- gg + geom_line(data = plot_df, aes(y = theoretical, color = "Teórica"),
                             linewidth = 1.2, linetype = "dashed")
    }
    
    return(gg)
}


sampling_posterior <- function(y, X, 
                                beta_bar = NULL, 
                                A = NULL, 
                                nu0 = 1, 
                                s0_sq = 1, 
                                n_samples = 10000) {
    X <- cbind(1, X)
    n <- nrow(X)
    k <- ncol(X)
    XtX <- crossprod(X)
    XtX_A <- XtX + A
    XtX_A_inv <- solve(XtX_A)
    beta_post_mean <- XtX_A_inv %*% (crossprod(X, y) + A %*% beta_bar)
    
    # Calcular término escalar S
    S <- nu0 * s0_sq + 
        crossprod(y) + 
        t(beta_bar) %*% A %*% beta_bar - 
        t(beta_post_mean) %*% XtX_A %*% beta_post_mean
    
    # Parámetros para la inversa-gamma
    shape <- (n + nu0) / 2
    scale <- S / 2
    
    # Almacenamiento de muestras
    samples <- list(
        beta = matrix(NA, nrow = n_samples, ncol = k),
        sigma2 = numeric(n_samples)
    )
    colnames(samples$beta) <- colnames(X)
    
    # Muestreo Gibbs
    for (i in 1:n_samples) {
        # Muestrear sigma²
        samples$sigma2[i] <- 1 / rgamma(1, shape = shape, rate = scale)
        
        # Muestrear beta
        samples$beta[i, ] <- MASS::mvrnorm(
            n = 1,
            mu = XtX_A_inv %*% (crossprod(X, y) + A %*% beta_bar),
            Sigma = samples$sigma2[i] * XtX_A_inv
        )
    }
    
    return(samples)
}

# Si no tienes instaladas las librerías:
# install.packages(c("ggplot2", "GGally", "viridis"))

plot_joint_posterior <- function(posterior_samples, alpha = 0.7, bins = 30) {
    beta_df <- as.data.frame(posterior_samples$beta)
    colnames(beta_df) <- c("Intercepto", "Pendiente")
    
    # Función personalizada para gráficos inferiores (2D density)
    lower_fn <- function(data, mapping, ...) {
        ggplot(data = data, mapping = mapping) +
            geom_density_2d_filled(aes(fill = after_stat(level)), alpha = alpha, bins = bins) +
            scale_fill_viridis_d(option = "magma") +
            theme_minimal()
    }
    
    # Función personalizada para la diagonal (densidad)
    diag_fn <- function(data, mapping, ...) {
        ggplot(data = data, mapping = mapping) +
            geom_density(fill = "#4B0082", alpha = 0.6, color = "black") +
            theme_minimal()
    }
    
    # Función personalizada para gráficos superiores (correlación)
    upper_fn <- function(data, mapping, ...) {
        ggally_cor(data = data, mapping = mapping, 
                   size = 5, color = "darkred", 
                   display_grid = FALSE) +
            theme_void()
    }
    
    # Crear la matriz de gráficos
    gg <- ggpairs(
        data = beta_df,
        lower = list(continuous = wrap(lower_fn)),
        diag = list(continuous = wrap(diag_fn)),
        upper = list(continuous = wrap(upper_fn)),
        progress = FALSE
    ) +
        theme(
            panel.grid = element_blank(),
            strip.background = element_rect(fill = "#2F4F4F"),
            strip.text = element_text(color = "white", face = "bold")
        )
    
    return(gg)
}

plot_sigma_posterior <- function(sigma_samples, 
                                 fill_color = "#4B0082", 
                                 line_color = "#2F4F4F",
                                 cred_level = 0.95,
                                 bins = 30,
                                 alpha = 0.7) {
    
    # Calcular intervalos de credibilidad
    cred_interval <- quantile(sigma_samples, 
                              probs = c((1 - cred_level)/2, 1 - (1 - cred_level)/2))
    
    # Crear el gráfico
    gg <- ggplot(data.frame(sigma2 = sigma_samples), aes(x = sigma2)) +
        geom_histogram(aes(y = after_stat(density)),
                       fill = fill_color,
                       color = line_color,
                       alpha = alpha,
                       bins = bins) +
        geom_density(color = line_color, 
                     linewidth = 1.2, 
                     adjust = 1.5) +
        geom_vline(xintercept = cred_interval,
                   color = "firebrick",
                   linetype = "dashed",
                   linewidth = 0.8) +
        annotate("text",
                 x = mean(cred_interval),
                 y = Inf,
                 label = paste0(cred_level*100, "% CI: [", 
                                round(cred_interval[1], 2), ", ",
                                round(cred_interval[2], 2), "]"),
                 vjust = 1.5,
                 hjust = 0.5,
                 color = "firebrick",
                 size = 4) +
        labs(title = "Distribución Posterior de σ²",
             x = expression(paste("Valor de ", sigma^2)),
             y = "Densidad") +
        theme_minimal(base_size = 14) +
        theme(
            plot.title = element_text(face = "bold", hjust = 0.5),
            axis.title = element_text(color = "#2F4F4F"),
            panel.grid.minor = element_blank(),
            panel.grid.major = element_line(linewidth = 0.1),
            plot.background = element_rect(fill = "white", color = NA)
        )
    
    return(gg)
}


####################################################################3

plot_marginal_theoretical <- function(posterior_samples, 
                                      X, y, 
                                      beta_bar, 
                                      A, 
                                      nu0, 
                                      s0_sq) {
    
    if (ncol(posterior_samples$beta) != 2) {
        stop("La función solo funciona para modelos con intercepto y un predictor.")
    }
    
    X <- cbind(1, X)
    n <- nrow(X)
    k <- ncol(X)
    XtX <- crossprod(X)
    XtX_A <- XtX + A
    XtX_A_inv <- solve(XtX_A)
    # Media condicional corregida
    beta_post_mean <- XtX_A_inv %*% (crossprod(X, y) + A %*% beta_bar)
    
    # Cálculo correcto del parámetro S
    S <- nu0 * s0_sq + 
        crossprod(y) + 
        t(beta_bar) %*% A %*% beta_bar - 
        t(beta_post_mean) %*% XtX_A %*% beta_post_mean
    
    # Grados de libertad
    nu <- n + nu0  
    
    # Rangos para graficar distribuciones
    x_range_beta0 <- seq(
        min(posterior_samples$beta[, 1]) - 2*sd(posterior_samples$beta[, 1]),
        max(posterior_samples$beta[, 1]) + 2*sd(posterior_samples$beta[, 1]),
        length.out = 200
    )
    
    x_range_beta1 <- seq(
        min(posterior_samples$beta[, 2]) - 2*sd(posterior_samples$beta[, 2]),
        max(posterior_samples$beta[, 2]) + 2*sd(posterior_samples$beta[, 2]),
        length.out = 200
    )
    
    x_range_sigma <- seq(
        0.1,
        max(posterior_samples$sigma2) + 2*sd(posterior_samples$sigma2),
        length.out = 200
    )
    
    # Densidades teóricas corregidas
    theoretical_densities <- list(
        beta0 = data.frame(
            x = x_range_beta0,
            y = dt(
                (x_range_beta0 - beta_post_mean[1]) / sqrt((S / nu) * XtX_A_inv[1,1]),  
                df = nu
            ) / sqrt((S / nu) * XtX_A_inv[1,1])
        ), 
        
        beta1 = data.frame(
            x = x_range_beta1,
            y = dt(
                (x_range_beta1 - beta_post_mean[2]) / sqrt((S / nu) * XtX_A_inv[2,2]), 
                df = nu
            ) / sqrt((S / nu) * XtX_A_inv[2,2])
        )
    )
    
    # Obtener la altura máxima del histograma de sigma2
    hist_data <- hist(posterior_samples$sigma2, plot = FALSE, breaks = 50) 
    max_hist_density <- max(hist_data$density)  # Máxima densidad observada en el histograma
    
    # Calcular la densidad teórica sin escalar
    raw_density <- dinvgamma(x_range_sigma, shape = nu/2, rate = S/2)
    
    # Escalar la densidad teórica para que tenga una altura comparable con el histograma
    scaling_factor <- max_hist_density / max(raw_density)
    
    # Aplicar el escalamiento
    theoretical_densities$sigma2 <- data.frame(
        x = x_range_sigma,
        y = raw_density * scaling_factor
    )
    
    # Gráficos
    plot_beta0 <- ggplot(data.frame(beta0 = posterior_samples$beta[,1]), aes(x = beta0)) +
        geom_histogram(aes(y = ..density..), bins = 50, fill = "#4B0082", alpha = 0.6) +
        geom_line(data = theoretical_densities$beta0, aes(x = x, y = y), 
                  color = "#FF6B6B", linewidth = 1.2) +
        labs(title = "Marginal de β₀", x = "β₀", y = "Densidad")
    
    plot_beta1 <- ggplot(data.frame(beta1 = posterior_samples$beta[,2]), aes(x = beta1)) +
        geom_histogram(aes(y = ..density..), bins = 50, fill = "#4B0082", alpha = 0.6) +
        geom_line(data = theoretical_densities$beta1, aes(x = x, y = y), 
                  color = "#FF6B6B", linewidth = 1.2) +
        labs(title = "Marginal de β₁", x = "β₁", y = "Densidad")
    
    plot_sigma2 <- ggplot(data.frame(sigma2 = posterior_samples$sigma2), aes(x = sigma2)) +
        geom_histogram(aes(y = ..density..), bins = 50, fill = "#2F4F4F", alpha = 0.6) +
        geom_line(data = theoretical_densities$sigma2, aes(x = x, y = y), 
                  color = "#2E8B57", linewidth = 1.2) +
        labs(title = "Marginal de σ²", x = "σ²", y = "Densidad")
    
    # Combinar gráficos
    grid.arrange(
        plot_beta0, 
        plot_beta1, 
        plot_sigma2,
        ncol = 2,
        layout_matrix = rbind(c(1, 2), c(3, 3))
    )
}

generate_data <- function(n = 50, beta0 = 0.8, beta1 = 1.3, sigma = 0.6) {
    x <- rnorm(n)
    y <- beta0 + beta1 * x + rnorm(n, sd = sigma)
    return(data.frame(x = x, y = y))
}

data <- generate_data()
```


# Regresión Lineal Simple

::: {.card title="Motivación: Ley de Igualación Generalizada" height=30%}

La ley de igualación generaliza de Baum (1974) nos indica que las tasas de respuesta o de tiempo dedicado a cada opción (programa de refuerzo) se relaciona con la tasa de reforzamiento de la siguiente forma: 

$$
\frac{B_1}{B_2} = \beta_0 (\frac{R_1}{R_2})^{\beta_1}
$$

Donde:

* $\frac{B_1}{B_2}$ representa la razón de las tasas de respuesta o el tiempo dedicado a cada opción.
* $\frac{R_1}{R_2}$ representan las tasas de reforzamiento en cada opción.
* $\beta_0$ es el sego que se tiene por la opción 1 en comparación a la opción 2.
* $\beta_1$ es la sensibilidad que se tiene a las diferencias en las tasas de reforzamiento.

Este modelo ha sido ampliamente utilizado para entender la conducta de elección y distribución del comportamiento tanto en humanos y otras especies. En los experimentos de elección en general tenemos dos opciones con distintos o mismos programas de refuerzo (IV, VI, FR, etc.) concurrentes y se mide la tasa de respuesta o tiempo dedicado a cada opción. Dado que la tasa de refuerzo es controlada, lo que se busca es estimar tanto el sesgo como la sensibilidad dadas otras condiciones experimentales (volatilidad del ambiente, contextos distintos, etc.).

Dada la forma funcional del modelo, estimar los parámetros sería complicado, pero usando las propiedades matemáticas del logarítmo, podemos transformarlo en un modelo lineal simple.

$$
log(\frac{B_1}{B_2}) = log(\beta_0) + \beta_1 log(\frac{R_1}{R_2})
$$
La interpretación de esta versión modificada del modelo permanece sin cambios, simplemente se tiene que tomar en cuenta que la escala de las variables cambia.

:::

::: {.card title="Estimación de Parámetros."}

Dado que nuestro modelo es una versión del modelo lineal simple, podemos estimar los parámetros por distintos métodos tales como mínimos cuadrados ordinarios (OLS, por sus siglas en inglés), máxima verosimilitud (MLE) o por métodos bayesianos. Para nuestro ejemplo usaremos el método bayesiano.

En el enfoque bayesiano se busca estimar la distribución de los parámetros dado los datos observados está distribución es conocida como posterior y es el resultado de combinar la información previa que se tiene de los parámetros con la información que se tiene de los datos observados dado nuestro modelo.

Definimos la posterior como:

$$
p(\Theta|x_1, \dots, x_n) \propto p(x_1, \dots, x_n| \Theta) p(\Theta)
$$

Donde: 

* $p(x_1, \dots, x_n| \Theta)$ es la verosimilitud de los datos dado los parámetros del modelo.
* $p(\Theta)$ es la distribución prior o inicial que representa nuestro conocimiento de los parámetros antes de observar los datos.

A pesar de la facilidad a prmera vista, el obtener la distribución posterior no es una tarea trivial y en general no tiene una forma analítica cerrada por lo que a menudo se recurre a métodos numéricos para obtener una aproximación a la distribución posterior, tales como MCMC (Markov Chain Monte Carlo) o métodos variacionales.

Sin embargo, en casos particulares es posible obtener expresiones tratables matemáticamente para la distribución posterior, en particular cuando dado un prior conjugado, la distribución posterior es de la misma familia que el prior, el prior conjugado no existe para todos los modelos, pero en nuestro caso existe.

Antes de ver el prior, es importante conocer nuestra función de verosimilitud, la cual para el modelo lineal simple es:

$$
p(y|X, \beta, \sigma^2) = \prod_{i=1}^{n} \frac{1}{\sqrt{2\pi \sigma^2}} exp(-\frac{1}{2\sigma^2}(y_i - \beta_0 - \beta_1 x_i)^2)
$$

Usando notación de algebra lineal, se puede reescribir como:

$$
p(y|X, \beta, \sigma^2) = \frac{1}{(2\pi \sigma^2)^{n/2}} exp(-\frac{1}{2\sigma^2}(y - X\beta)^T(y - X\beta))
$$

Nuestro prior conjugado es una distribución normal multivariada para los parámetros $\beta$ y una distribución gamma inversa para la varianza $\sigma^2$, la distribución completa es simplemente el producto de ambos, pero cabe destacar que la distribución de los parámetros $\beta$ depende de la vairanza $\sigma^2$. Podemos expresarlo como:

$$
p(\beta, \sigma^2) = p(\beta|\sigma^2) p(\sigma^2)
$$

$$
p(\beta, \sigma^2) = N_n (\beta|\beta_0, \sigma^2 \Sigma) IG(\sigma^2|\nu_0, s_0^2)
$$

Usando su densidad de probabilidad, tenemos:

$$
p(\beta, \sigma^2) = \frac{1}{(2\pi)^{k/2} |\Sigma|^{1/2}} exp(-\frac{1}{2}(\beta - \beta_0)^T \Sigma^{-1} (\beta - \beta_0)) \times \frac{1}{B(\frac{\nu_0}{2}, \frac{\nu_0 s_0^2}{2})} (\frac{\nu_0 s_0^2}{2})^{\frac{\nu_0}{2}} (\sigma^2)^{-\frac{\nu_0 + k + 2}{2}} exp(-\frac{\nu_0 s_0^2}{2 \sigma^2})
$$


Usando la verosimilitud y el prior conjugado, se puede demostrar que la distrobución posterior es una distribución normal multivariada para los parámetros $\beta$ y una distribución gamma inversa para la varianza $\sigma^2$. Los parámetros actualizdos para la posterior son:
\begin{align*}
\mu_{\beta} &= (X^TX + A)^{-1}(X^Ty + A\beta_0) \\
\sigma^2 &= \frac{S}{n + \nu_0} \\
\nu_n &= n + \nu_0 \\
s_n^2 &= \frac{S}{2} \\
S &= \nu_0 s_0^2 + y^Ty + \beta_0^T A \beta_0 - \mu_{\beta}^T(X^TX + A)\mu_{\beta}
\end{align*}

Donde $A$ es la matriz de covarianza de los parámetros $\beta$ y $\nu_0$ y $s_0^2$ son los hiperparámetros de la distribución gamma inversa. Nuestra distribución posterior es:

$$
p(\beta, \sigma^2|y, X) = N_{k}(\beta|\mu_{\beta}, \sigma^2 (X^TX + A)^{-1}) IG(\sigma^2|\nu_n, s_n^2)
$$
:::



# App

##  {.sidebar}

```{r}
actionButton("run_sampling", "Muestrear")

br()
```

```{r}
numericInput(
    "n_samples", 
    "Tamaño de Muestra Posterior", 
    value = 1000, 
    min = 10, 
    max = 10000,
    step = 25
  )

numericInput(
    "alpha", 
    "Tamaño de HDI", 
    value = 0.90, 
    min = 0.70, 
    max = 0.99,
    step = 0.01
  )

numericInput(
    "prior_mean_b0", 
    "Media Intercepto", 
    value = 0, 
    min = -10, 
    max = 10,
    step = 0.1
  )

numericInput(
    "prior_mean_b1", 
    "Media Pendiente", 
    value = 0, 
    min = -10, 
    max = 10,
    step = 0.1
  )

numericInput(
    "sd_mean_b0", 
    "Desviación Est. Intercepto", 
    value = 1, 
    min = 0.01, 
    max = 20,
    step = 0.1
  )

numericInput(
    "sd_mean_b1", 
    "Desviación Est. Pendiente", 
    value = 1, 
    min = 0.01, 
    max = 20,
    step = 0.1
  )

numericInput(
    "prior_corr", 
    "Correlación entre Intercepto y Pendiente", 
    value = 0, 
    min = -1, 
    max = 1,
    step = 0.05
  )

br()
```

```{r}
sliderInput("df_chi", "Grados de Libertad", 
              min = 1, max = 30, value = 1, step = 1)

sliderInput("scale_chi", "Escala", 
              min = 1, max = 20, value = 1, step = 0.1)
```

## Datos {height="35%"}

```{r}
#| context: server

output$plotdata <- renderPlot({
    
    p <- ggplot(data = data, 
                aes(x = x, y = y)) + 
        geom_point(color = "#55A154", alpha = 0.35, size = 3) +
        labs(
            title = "",
            y = expression(log(frac(plain(B)[1], plain(B)[2]))),
            x = expression(log(frac(plain(R)[1], plain(R)[2]))
            ))
    p
    })


posterior_data <- eventReactive(input$run_sampling, {
    
    if (nrow(data) == 0) return(NULL)  # Evita error si los datos no están cargados
    
    X <- data$x
    y <- data$y
    
    betas_mean <- matrix(c(input$prior_mean_b0, input$prior_mean_b1), nrow = 2, ncol = 1)
    
    cov_betas <- input$prior_corr * input$sd_mean_b0 * input$sd_mean_b1
    
    sigma <- matrix(c(input$sd_mean_b0^2, cov_betas, 
                      cov_betas, input$sd_mean_b1^2), 
                    nrow = 2, ncol = 2)
    
    samples <- sampling_posterior(
        y, X, betas_mean, 
        sigma, input$df_chi, input$scale_chi, 
        n_samples = input$n_samples
    )
    
    list(samples = samples, betas_mean = betas_mean, sigma = sigma)
})

output$plotpostfitdata <- renderPlot({
    posterior <- posterior_data()
    if (is.null(posterior)) return(NULL)  # Evita error si posterior_data aún no existe
    
    X <- data$x
    y <- data$y
    
    plot_bayes_regression(
        y, X, posterior$samples,  # Extraer muestras
        posterior$betas_mean,      # Extraer media
        posterior$sigma,           # Extraer sigma
        input$df_chi, input$scale_chi,
        input$alpha
    )
})

output$plot_betas_postconj <- renderPlot({
    
    posterior <- posterior_data()
    if (is.null(posterior)) return(NULL)  # Evita error si posterior_data aún no existe
    
    plot_joint_posterior(posterior$samples, alpha = 0.7, bins = 30)
})

output$plot_sigma_post <- renderPlot({
    
    posterior <- posterior_data()
    if (is.null(posterior)) return(NULL)  # Evita error si posterior_data aún no existe
    
    plot_sigma_posterior(posterior$samples$sigma2, alpha = 0.7, bins = 30, cred_level = input$alpha)
})

output$plot_marginales <- renderPlot({
    
    posterior <- posterior_data()
    if (is.null(posterior)) return(NULL)  # Evita error si posterior_data aún no existe
    
    X <- data$x
    y <- data$y
    
    plot_marginal_theoretical(posterior$samples, X, y, 
                              posterior$betas_mean,      # Extraer media
                              posterior$sigma,           # Extraer sigma
                              input$df_chi, input$scale_chi) 
})
```

### Plots

```{r}
#| title: "Datos"
plotOutput('plotdata')
```

```{r}
#| title: "Estimación"
plotOutput('plotpostfitdata')
```

### Row

```{r}
#| title: "Conjunta Intercepto, Pendiente"
plotOutput('plot_betas_postconj')
```

```{r}
#| title: "Marginal para la Varianza"

plotOutput("plot_sigma_post")
```

### Row

```{r}
#| title: "Marginales Teóricas vs Muestreadas"
plotOutput("plot_marginales")
```

# Demostraciones

ahjsbfhjkbas
